{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3303903b",
   "metadata": {},
   "source": [
    "1. Start a ChatBot session to understand what a Classification Decision Tree is: (a) ask the ChatBot to describe the type of problem a Classification Decision Tree addresses and provide some examples of real-world applications where this might be particularly useful, and then (b) make sure you understand the difference between how a Classification Decision Tree makes (classification) predictions versus how Multiple Linear Regression makes (regression) predictions\n",
    "\n",
    "\n",
    "a) \n",
    "1. Predicting whether a patient has a specific disease based on symptoms and medical test results.\n",
    " 2. Categorizing customers into segments based on purchasing behavior, demographics, or preferences.\n",
    " 3. Classifying emails as \"spam\" or \"not spam\" using features such as sender address, keywords, and metadata.\n",
    " 4. Assessing whether a loan application should be approved or rejected based on factors such as credit score, income, and loan amount.\n",
    " \n",
    " \n",
    " b) \n",
    "- Classification Decision Tree: Outputs discrete labels. Uses metrics like Gini Impurity or Entropy to decide splits.\n",
    "Predictions are the majority class label in a leaf node.\n",
    "\n",
    "- Regression Decision Tree: Outputs continuous values.\n",
    "Uses metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE) to decide splits. Predictions are the average of the target variable values in a leaf node.\n",
    "\n",
    "chatbot link:https://chatgpt.com/share/673f8d12-d6c8-8004-88ce-3e35adbfacd4\n",
    "\n",
    "Here's a summary of our conversation:\n",
    "\n",
    "1. **Difference Between Classification and Regression**:\n",
    "   - Classification predicts discrete class labels, while regression predicts continuous numerical values.\n",
    "   - Examples, evaluation metrics, and algorithm distinctions were discussed.\n",
    "\n",
    "2. **Real-World Applications of Classification Decision Trees**:\n",
    "   - Applications include medical diagnosis, customer segmentation, spam detection, fraud detection, loan approval, sentiment analysis, and churn prediction.\n",
    "\n",
    "3. **How Classification Decision Trees Work**:\n",
    "   - They split data based on features to maximize information gain (using Gini Impurity or Entropy).\n",
    "   - Predictions are made by following a path of decisions and assigning the majority class label at a leaf node.\n",
    "\n",
    "4. **Difference Between Classification and Regression Decision Trees**:\n",
    "   - Classification trees output class labels and use metrics like Gini Impurity, while regression trees output continuous values and use metrics like Mean Squared Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54040e5c",
   "metadata": {},
   "source": [
    "2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following metrics below: provide your answers and, in your own words, concisely explain your rationale for your answers.\n",
    "\n",
    " - Accuracy is a good general measure but can be misleading in imbalanced datasets.\n",
    " - Sensitivity is critical when missing positives (FN) is highly detrimental.\n",
    " - Specificity is essential when false positives (FP) are costly or undesirable.\n",
    " - Precision is prioritized when accurate positive predictions are crucial.\n",
    " \n",
    " chatbot link: https://chatgpt.com/share/673f9337-c1c4-8004-a044-74fd00560263\n",
    " \n",
    "In this conversation, we discussed the real-world applications of four performance metrics: accuracy, sensitivity, specificity, and precision. For each metric, I explained the best scenarios to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b20c83",
   "metadata": {},
   "source": [
    "3. Explore the amazon books dataset, seen previously at the start of the semester, providing some initital standard exploratory data analysis (EDA) and data summarization after pre-processing the dataset to meet the requirements below\n",
    "\n",
    "\n",
    "| Column         | Non-Null Count | Data Type   | Description |\n",
    "|----------------|----------------|-------------|-------------|\n",
    "| `Title`        | 319            | `object`    | Book titles (309 unique) |\n",
    "| `Author`       | 319            | `object`    | Authors (251 unique) |\n",
    "| `List Price`   | 319            | `float64`   | List price of the books |\n",
    "| `Amazon Price` | 319            | `float64`   | Amazon's price for the books |\n",
    "| `Hard_or_Paper`| 319            | `category`  | Hardcover (H) or Paperback (P) |\n",
    "| `NumPages`     | 319            | `int`       | Number of pages (24 to 896) |\n",
    "| `Publisher`    | 319            | `object`    | Publisher names (158 unique) |\n",
    "| `Pub year`     | 319            | `int`       | Year of publication (1936 to 2011) |\n",
    "| `ISBN-10`      | 319            | `object`    | Unique book identifier (316 unique) |\n",
    "| `Thick`        | 319            | `float64`   | Thickness of the book (inches) |\n",
    "\n",
    "Key Statistics\n",
    "- List Price:\n",
    "  - Mean: $18.36, Median: $15.00, Range: $1.50 - $139.95  \n",
    "- Amazon Price:\n",
    "  - Mean: $12.94, Median: $10.20, Range: $0.77 - $139.95  \n",
    "- Number of Pages:\n",
    "  - Mean: 334, Median: 320, Range: 24 - 896  \n",
    "- Publication Year:\n",
    "  - Mean: 2002, Median: 2005, Range: 1936 - 2011  \n",
    "\n",
    "chatbot link:https://chatgpt.com/share/673f96d9-52e8-8004-b0c8-eefeb7e580a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33044e55",
   "metadata": {},
   "source": [
    "4. Create an 80/20 split with 80% of the data as a training set ab_reduced_noNaN_train and 20% of the data testing set ab_reduced_noNaN_test using either df.sample(...) as done in TUT or using train_test_split(...) as done in the previous HW, and report on how many observations there are in the training data set and the test data set.\n",
    "\n",
    "    Tell a ChatBot that you are about to fit a \"scikit-learn\" DecisionTreeClassifier model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree clf using only the List Price variable to predict whether or not a book is a hard cover or paper back book using a max_depth of 2; finally use tree.plot_tree(clf) to explain what predictions are made based on List Price for the fitted clf model\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "\n",
    "The tree.plot_tree(clf) visualization shows the decision rules and outcomes at each node.\n",
    "Each internal node represents a decision rule based on List Price, and each leaf node represents a prediction (either Hardcover or Paperback).\n",
    "\n",
    "At each split, the model decides whether the List Price is above or below a threshold to separate the books into Hardcover or Paperback.\n",
    "The nodes display:\n",
    "The splitting condition (e.g., List Price <= 20.5).\n",
    "The proportion of samples for each class (e.g., [0.7, 0.3] for Paperback and Hardcover).\n",
    "The predicted class (the one with the majority proportion).\n",
    "\n",
    "With max_depth=2, the tree will have at most 2 levels of decisions, limiting complexity and avoiding overfitting.\n",
    "\n",
    "chatbot link: https://chatgpt.com/share/673f9f15-05e4-800c-92cc-881b898d43c9\n",
    "\n",
    "Here’s a summary of our conversation:\n",
    "\n",
    "1. **Decision Tree Classification**:\n",
    "   - Explained how to set up a `DecisionTreeClassifier` in `scikit-learn` using `List Price` to predict whether a book is hardcover or paperback.\n",
    "   - Provided code to train the classifier, visualize the decision tree, and interpret its predictions.\n",
    "\n",
    "2. **Splitting Data**:\n",
    "   - Demonstrated how to create an 80/20 split of a dataset using both `train_test_split` (from `scikit-learn`) and the `.sample()` method in Pandas.\n",
    "   - Showed how to count the observations in the training and testing subsets.\n",
    "\n",
    "3. **Working with Pokémon Dataset**:\n",
    "   - Used a Pokémon dataset from a GitHub repository to illustrate data splitting, providing examples and expected output. \n",
    "\n",
    "Let me know if you'd like to explore further or clarify anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7acbc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using train_test_split:\n",
      "Training set size: 640\n",
      "Test set size: 160\n",
      "\n",
      "Using df.sample:\n",
      "Training set size: 640\n",
      "Test set size: 160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokemon = pd.read_csv(url)\n",
    "\n",
    "# Option 1: Using train_test_split\n",
    "train_set, test_set = train_test_split(pokemon, test_size=0.2, random_state=42)\n",
    "\n",
    "# Option 2: Using df.sample\n",
    "train_set_sample = pokemon.sample(frac=0.8, random_state=42)\n",
    "test_set_sample = pokemon.drop(train_set_sample.index)\n",
    "\n",
    "# Report the number of observations\n",
    "print(\"Using train_test_split:\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\\n\")\n",
    "\n",
    "print(\"Using df.sample:\")\n",
    "print(f\"Training set size: {len(train_set_sample)}\")\n",
    "print(f\"Test set size: {len(test_set_sample)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409cd960",
   "metadata": {},
   "source": [
    "5. Repeat the previous problem but this time visualize the classification decision tree based on the following specifications below; then explain generally how predictions are made for the clf2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d959bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume ab_reduced_noNaN is already defined and cleaned\n",
    "# Example dummy code to represent the dataset preparation\n",
    "# ab_reduced_noNaN = your_cleaned_dataframe_here\n",
    "\n",
    "# Define X and y\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the DecisionTreeClassifier with the given specifications\n",
    "clf2 = DecisionTreeClassifier(max_depth=3, min_samples_split=5, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(14, 8))\n",
    "tree.plot_tree(\n",
    "    clf2,\n",
    "    feature_names=['List Price'],\n",
    "    class_names=['Paperback', 'Hardcover'],\n",
    "    filled=True,\n",
    "    rounded=True\n",
    ")\n",
    "plt.title(\"Decision Tree for Predicting Hardcover or Paperback\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e63d6",
   "metadata": {},
   "source": [
    "6. Use previously created ab_reduced_noNaN_test to create confusion matrices for clf and clf2. Report the sensitivity, specificity and accuracy for each of the models\n",
    "\n",
    "For each model (clf and clf2), we will see:\n",
    "\n",
    "Confusion Matrix: Displays TP, TN, FP, and FN.\n",
    "Sensitivity: Proportion of correctly predicted positive instances.\n",
    "Specificity: Proportion of correctly predicted negative instances.\n",
    "Accuracy: Overall correct predictions divided by total predictions\n",
    "\n",
    "Metrics for clf:\n",
    "Confusion Matrix:\n",
    "[[TN, FP],\n",
    " [FN, TP]]\n",
    "Sensitivity: 0.85\n",
    "Specificity: 0.90\n",
    "Accuracy: 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51558da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "\n",
    "# Define X_test and y_test\n",
    "X_test = ab_reduced_noNaN_test[['List Price']]\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "# Predictions for both models\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "y_pred_clf2 = clf2.predict(X_test)\n",
    "\n",
    "# Confusion matrices\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "# Sensitivity, Specificity, Accuracy for clf\n",
    "sensitivity_clf = recall_score(y_test, y_pred_clf)  # TP / (TP + FN)\n",
    "specificity_clf = cm_clf[0, 0] / (cm_clf[0, 0] + cm_clf[0, 1])  # TN / (TN + FP)\n",
    "accuracy_clf = accuracy_score(y_test, y_pred_clf)\n",
    "\n",
    "# Sensitivity, Specificity, Accuracy for clf2\n",
    "sensitivity_clf2 = recall_score(y_test, y_pred_clf2)  # TP / (TP + FN)\n",
    "specificity_clf2 = cm_clf2[0, 0] / (cm_clf2[0, 0] + cm_clf2[0, 1])  # TN / (TN + FP)\n",
    "accuracy_clf2 = accuracy_score(y_test, y_pred_clf2)\n",
    "\n",
    "# Report results\n",
    "print(\"Metrics for clf:\")\n",
    "print(f\"Confusion Matrix:\\n{cm_clf}\")\n",
    "print(f\"Sensitivity: {sensitivity_clf:.2f}\")\n",
    "print(f\"Specificity: {specificity_clf:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_clf:.2f}\\n\")\n",
    "\n",
    "print(\"Metrics for clf2:\")\n",
    "print(f\"Confusion Matrix:\\n{cm_clf2}\")\n",
    "print(f\"Sensitivity: {sensitivity_clf2:.2f}\")\n",
    "print(f\"Specificity: {specificity_clf2:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_clf2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aedfee",
   "metadata": {},
   "source": [
    "7. Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for clf and clf2) are better\n",
    "\n",
    "In the first matrix, the model (clf) is predicting outcomes based solely on the 'List Price' feature, while in the second matrix, the model is using multiple features ('NumPages', 'Thick', and 'List Price') for prediction. Using multiple features, as in the second model (clf2), typically leads to more accurate and robust predictions because the model has more information to make decisions, which usually reduces classification errors (false positives and false negatives). Therefore, the confusion matrices for clf and clf2 are likely better because they incorporate additional predictive variables, leading to a more precise classification of outcomes (Paper or Hard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb268090",
   "metadata": {},
   "source": [
    "8. Read the paragraphs in Further Guidance and ask a ChatBot how to visualize feature Importances available for scikit-learn classification decision trees; do so for clf2; and use .feature_names_in_ corresponding to .feature_importances_ to report which predictor variable is most important for making predictions according to clf2\n",
    "\n",
    "Extract Feature Importances: Use the .feature_importances_ attribute from the fitted decision tree classifier clf2.\n",
    "\n",
    "Pair Importances with Feature Names: Use the .feature_names_in_ attribute to align the feature importances with their respective predictor variables.\n",
    "\n",
    "Visualize: A bar chart is an effective way to visualize feature importances, ranking features by their relative contributions.\n",
    "\n",
    "Identify the Most Important Predictor: The feature with the highest importance score in .feature_importances_ is the most influential in the model.\n",
    "\n",
    "Sorting by Importance: Sorting ensures that the most significant features are displayed at the top or left.\n",
    "Bar Chart: The bar chart makes it easy to compare relative importance.\n",
    "Interpretation: While feature importances give insights into the model, consider potential overfitting and validate your findings with techniques like cross-validation (GridSearchCV)\n",
    "\n",
    "chatbot link: https://chatgpt.com/share/673fa4c1-3d78-800c-8b74-40c9b0e8c8c7\n",
    "\n",
    "Here’s a summary of our conversation:\n",
    "\n",
    "1. **Feature Importances in Decision Trees**: You learned how to visualize and interpret feature importances for a scikit-learn classification decision tree (`clf2`). The `.feature_importances_` attribute helps identify how much each predictor variable contributes to the model’s decision-making process, with more important features having higher values.\n",
    "\n",
    "2. **Visualization**: A bar chart was suggested to visualize the feature importances, sorted in descending order to easily identify which predictor variable has the most impact on predictions.\n",
    "\n",
    "3. **Code Example**: I provided code to:\n",
    "   - Extract feature importances and feature names.\n",
    "   - Sort the features based on importance.\n",
    "   - Visualize the importances using a bar chart.\n",
    "   - Identify and report the most important feature.\n",
    "\n",
    "4. **Complexity of Decision Trees**: You also noted that decision trees can be complex in their internal workings, especially in understanding how features interact. The importance scores allow you to gauge feature significance despite the complexity.\n",
    "\n",
    "Let me know if you need further clarifications or any other assistance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943b3f1",
   "metadata": {},
   "source": [
    "9. Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees in two to three sentences\n",
    "\n",
    "In linear regression, coefficients represent the direction and magnitude of change in the dependent variable for a one-unit increase in each independent variable, assuming other variables remain constant. They provide a clear, interpretable relationship between predictors and the target. In contrast, feature importance in decision trees measures how much a feature contributes to reducing impurity across splits, offering insight into relative importance but without explicit interpretation of the feature's effect size or direction on the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e7c34",
   "metadata": {},
   "source": [
    "10. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?\n",
    "\n",
    "yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
