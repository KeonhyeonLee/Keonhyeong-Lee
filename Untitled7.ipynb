{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2043fde3",
   "metadata": {},
   "source": [
    "1. The \"first pre-lecture video\" (above) describes hypothesis testing as addressing \"an idea that can be tested\", and the end of the video then discusses what our actual intended purpose in setting up a null hypothesis is. What is the key factor that makes the difference between ideas that can, and cannot be examined and tested statistically? What would you describe is the key \"criteria\" defining what a good null hypothesis is? And what is the difference between a null hypothesis and an alternative hypothesis in the context of hypothesis testing? Answer these questions with concise explanations in your own words.\n",
    "\n",
    "\n",
    "Difference between ideas that can, and cannot be examined is whether the idea can be expressed in a measurable and falsifiable form \n",
    "ex) my height is 6ft\n",
    "\n",
    "The key \"criteria\" defining what a good null hypothesis is an idea can be statistically tested and can be expressed in a measurable and falsifiable form\n",
    "\n",
    "The null hypothesis : It assumes no change, effect, or relationship between variables\n",
    "The alternative hypothesis : It suggests that there is a significant effect or relationship, representing the researcher's prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa898a38",
   "metadata": {},
   "source": [
    "2. Towards the end of the \"first pre-lecture\" video (above) it is stated that, \"It is important to note that outcomes of tests refer to the population parameter, rather than the sample statistic! As such, the result that we get is for the population.\" In terms of the distinctions between the concepts of  𝑥𝑖\n",
    " 's,  𝑥¯\n",
    " ,  𝜇\n",
    " , and  𝜇0\n",
    " , how would you describe what the sentence above means? Explain this concisely in your own words for a \"non-statsitical\" audience, defining the technical statistical terminology you use in your answer.\n",
    " \n",
    "When a test is maded, we are really trying to make an inference about the true population mean (true average of something for an entire group of interest) based on the sample mean (average we calculate from a smaller group, that we can actually measure) rather than just saying something about our particular sample. \n",
    "This is important since we care about the wide range of population, not just the specific group we collected data from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d866c",
   "metadata": {},
   "source": [
    "3. The second \"Pre-lecture\" video (above) explains that we \"imagine a world where the null hypothesis is true\" when calculating a p-value? Explain why this is in your own words in a way that makes the most sense to you.\n",
    "\n",
    "When calculating a p-value, we imagine a world where the null hypothesis is true just like what the video said. Because we need a reference to compare our data. The null hypothesis is like a starting assumption that says, “nothing unusual is happening” or \"there's no effect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f3bbe",
   "metadata": {},
   "source": [
    "4. The second \"Pre-lecture\" video (above) describes suggest that a smaller p-value makes the null hypothesis look more ridiculous. Explain why this is in your own words in a way that makes the most sense to you, clarifying the meaning of any technical statistical terminology you use in your answer.\n",
    "\n",
    "A smaller p-value makes the null hypothesis look more ridiculous. Because it suggests that the data will be very unlikely to be true if thenull hypothesis were actually true. As the p-value become smaller, the evidence that tell the null hypothesis is wrong become stronger, which making us more confident in rejecting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2bca5",
   "metadata": {},
   "source": [
    "5. Güntürkün (2003) recorded how kissing couples tilt their heads. 80 out of 124 couples, or 64.5% tilted their heads to the right. Simulate a p-value using a \"50/50 coin-flipping\" model for the assumption of the null hypothesis  𝐻0 that the population of humans don't have left or right head tilt tendencies when kissing, and use the table below to determine the level of evidence we have against  𝐻0.\n",
    "\n",
    "I got 0.0007 from the test, which means \"Weak evidence against the null hypothesis\"\n",
    "\n",
    "chatbot summary\n",
    "In this conversation, you asked for a Python code to calculate a p-value based on a \"50/50 coin-flipping\" model, with parameters including 124 couples, 80 observed right tilts, and 10,000 simulations. I provided a Python code that simulates the null hypothesis using binomial trials to represent the 50/50 chance of a right tilt for each couple. The code calculates the p-value by determining how many simulations result in at least as many right tilts as observed (80 out of 124), then computes the proportion of these simulations relative to the total number (10,000).\n",
    "\n",
    "Let me know if you'd like to dive deeper into any of these steps!\n",
    "\n",
    "Chatbot link:https://chatgpt.com/share/67076984-25f0-8004-a4bb-84d6c7971597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a707f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.0007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_p_value(n_couples=124, observed_right_tilts=80, num_simulations=10000):\n",
    "    # Simulate coin flips under the null hypothesis (50% chance for heads)\n",
    "    simulations = np.random.binomial(n_couples, 0.5, size=num_simulations)\n",
    "\n",
    "    # Calculate how many simulations are at least as extreme as observed\n",
    "    extreme_count = np.sum(simulations >= observed_right_tilts)\n",
    "\n",
    "    # Calculate the p-value\n",
    "    p_value = extreme_count / num_simulations\n",
    "    return p_value\n",
    "\n",
    "# Parameters\n",
    "n_couples = 124\n",
    "observed_right_tilts = 80\n",
    "num_simulations = 10000\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = calculate_p_value(n_couples, observed_right_tilts, num_simulations)\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ac8d4",
   "metadata": {},
   "source": [
    "6. Can a smaller p-value definitively prove that the null hypothesis is false? Is it possible to definitively prove that Fido (from the \"second pre-lecture video\") is innocent using a p-value? Is it possible to difinitively prove that Fido is guilty using a p-value? How low or high does a p-value have to be to definitely prove one or the other? Explain this concisely in your own words\n",
    "\n",
    "NO, A smaller p-value cannot prove that the null hypothesis is false. Bacause, it only gives us a probability-based measure of how consistent the data is with the null hypothesis. Statistics works with evidence, not absolute certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d6b22",
   "metadata": {},
   "source": [
    "7. In the second half of the \"first pre-lecture video\" the concept of a \"one sided\" (or \"one tailed\") test is introduced in contrast to a \"two sided\" (or \"two tailed\") test. Work with a ChatBot to adjust the code from \"Demo II of the Week 5 TUT\" (which revisits the \"Vaccine Data Analysis Assignment\" from Week 04 HW \"Question 8\") in order to compute a p-value for a \"one sided\" (or \"one tailed\") hypothesis test rather than the \"two sided\" (or \"two tailed\") version it provides. Describe (perhaps with the help of your ChatBot) what changed in the code; how this changes the interpretation of the hypothesis test; and whether or not we should indeed expect the p-value to be smaller in the \"one tailed\" versus \"two tailed\" analysis.\n",
    "\n",
    "P-value calculation was added to assess the statistical significance of the observed means.\n",
    "\n",
    "In general, we have to expect a smaller p-value in a one-tailed test compared to a two-tailed test by assuming the same observed data. Because the one-tailed test is more focused on detecting an effect in a specific direction. This makes it easier to reach statistical significance if the effect exists in the predicted direction.\n",
    "\n",
    "\n",
    "chatbot summary\n",
    "Here's a summary of our conversation:\n",
    "\n",
    "1. **Bootstrap Resampling and Visualization**: \n",
    "   - You shared Python code to perform bootstrap resampling on a dataset with `Age`, `Score1`, and `Score2` columns. The code calculated the mean for each bootstrap sample and plotted histograms of these means.\n",
    "\n",
    "2. **P-value Calculation**: \n",
    "   - You requested the code to calculate p-values from the bootstrap samples. The code was modified to:\n",
    "     - Calculate observed means for each variable (`Age`, `Score1`, `Score2`).\n",
    "     - Add a function to compute p-values by comparing the observed means to the bootstrap distributions (using a two-tailed test).\n",
    "     - Visualize the observed means with red dashed lines in the histograms, along with the calculated p-values.\n",
    "\n",
    "3. **One-Tailed vs. Two-Tailed Tests**:\n",
    "   - We discussed the difference between one-tailed and two-tailed p-values:\n",
    "     - A **two-tailed test** checks for differences in both directions (greater or less than the null value), leading to a larger p-value.\n",
    "     - A **one-tailed test** only checks for a difference in one direction, leading to a smaller p-value if the effect is in that direction.\n",
    "   - The p-value is typically smaller in a one-tailed test because it only focuses on one side of the distribution.\n",
    "\n",
    "Let me know if you'd like further clarifications!\n",
    "\n",
    "chatbot link:https://chatgpt.com/share/6707f38e-1928-8004-b584-57ce1ae68849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42568124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value for Age: 0.98\n",
      "P-value for Score1: 1.0\n",
      "P-value for Score2: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Age': [45, 34, 29, 52, 37, 41, 33, 48, 26, 39],\n",
    "    'Gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'],\n",
    "    'Score1': [84, 78, 83, 81, 81, 80, 79, 85, 76, 83],\n",
    "    'Score2': [86, 86, 80, 86, 84, 86, 86, 82, 83, 84]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Observed means from the original data\n",
    "observed_mean_age = df['Age'].mean()\n",
    "observed_mean_score1 = df['Score1'].mean()\n",
    "observed_mean_score2 = df['Score2'].mean()\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "n_bootstrap_samples = 1000\n",
    "\n",
    "# Arrays to store the mean of each bootstrap sample\n",
    "bootstrap_means_age = []\n",
    "bootstrap_means_score1 = []\n",
    "bootstrap_means_score2 = []\n",
    "\n",
    "# Generate bootstrap samples and calculate means\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    sample = df.sample(frac=1, replace=True)  # sample with replacement\n",
    "    bootstrap_means_age.append(sample['Age'].mean())\n",
    "    bootstrap_means_score1.append(sample['Score1'].mean())\n",
    "    bootstrap_means_score2.append(sample['Score2'].mean())\n",
    "\n",
    "# Convert bootstrap mean lists to numpy arrays for calculations\n",
    "bootstrap_means_age = np.array(bootstrap_means_age)\n",
    "bootstrap_means_score1 = np.array(bootstrap_means_score1)\n",
    "bootstrap_means_score2 = np.array(bootstrap_means_score2)\n",
    "\n",
    "# Calculate p-values for two-tailed test\n",
    "def calculate_p_value(bootstrap_means, observed_mean):\n",
    "    extreme_count = np.sum(np.abs(bootstrap_means - np.mean(bootstrap_means)) >= np.abs(observed_mean - np.mean(bootstrap_means)))\n",
    "    p_value = extreme_count / len(bootstrap_means)\n",
    "    return p_value\n",
    "\n",
    "# Calculate p-values for each variable\n",
    "p_value_age = calculate_p_value(bootstrap_means_age, observed_mean_age)\n",
    "p_value_score1 = calculate_p_value(bootstrap_means_score1, observed_mean_score1)\n",
    "p_value_score2 = calculate_p_value(bootstrap_means_score2, observed_mean_score2)\n",
    "\n",
    "# Print the p-values\n",
    "print(f\"P-value for Age: {p_value_age}\")\n",
    "print(f\"P-value for Score1: {p_value_score1}\")\n",
    "print(f\"P-value for Score2: {p_value_score2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1288af9",
   "metadata": {},
   "source": [
    "8. Complete the following assignment.\n",
    "\n",
    "- Relationship between this Experiment and Fisher and Bristol's Original Experiment : \n",
    "The main difference is the number of participants and cups tested. Fisher and Bristol's experiment was 8 cups, while our experiment involves 80 students tasting one cup each. Fisher's hypothesis test design is the foundation for our analysis, where we test if students’ correct identification rate differs from random guessing.\n",
    "\n",
    " - Statements of the Null Hypothesis and Alternative Hypothesis : \n",
    "Null Hypothesis: Students are randomly guessing whether the milk or tea was poured first. Thus, the probability of correctly identifying the pouring order is 50% (p=0.5.).\n",
    "Alternative Hypothesis: Students are not randomly guessing, and they are better than 50% at identifying the pouring order (𝑝>0.5).\n",
    "\n",
    "- Quantitative Analysis : \n",
    "I will conduct a one-sample proportion test to compare the observed proportion of correct answers 49 out of 80 to the hypothesized proportion of 50% under the null hypothesis.\n",
    "\n",
    " - Methodology Code and Explanations : \n",
    "see code below\n",
    "\n",
    "chatbot summary:\n",
    "You asked for a Python code to calculate the p-value, test the null hypothesis, and compute the sample proportion based on a dataset where 80 students were tested and 49 answered correctly, with the null hypothesis proportion (\\( p_0 \\)) being 0.5.\n",
    "I provided you with a Python script that:\n",
    "  Calculates the sample proportion (\\( \\hat{p} \\)).\n",
    "  Computes the z-score to test the difference between the sample proportion and the null hypothesis.\n",
    "  Calculates the p-value using a two-tailed z-test.\n",
    "  Decides whether to reject or fail to reject the null hypothesis based on the p-value and a significance level (\\( \\alpha \\)) of 0.05. Would you like to proceed with anything else?\n",
    "chatbot link:https://chatgpt.com/share/670826f5-1434-8004-be11-52f9459099d9\n",
    "\n",
    " - Findings and Discussion\n",
    "Sample proportion: 0.6125 (61.25%)\n",
    "Z-statistic: 2.01\n",
    "P-value: 0.022, which is smaller than the significance level \n",
    "α = 0.05  means we have sufficient evidence to reject the null hypothesis. The result suggests that the students are likely better than random guessing at identifying the pouring order of tea and milk. However, while we reject the null hypothesis, the evidence is not overwhelmingly strong as the p-value is close to 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c32478fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample proportion (p̂): 0.6125\n",
      "Z-statistic: 2.0124611797498115\n",
      "P-value: 0.022085672454221217\n",
      "Reject the null hypothesis (H₀).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Make the analysis reproducible\n",
    "np.random.seed(42)\n",
    "\n",
    "# Given data\n",
    "n_students = 80  # number of students\n",
    "correct_answers = 49  # number of correct answers\n",
    "p_hat = correct_answers / n_students  # sample proportion\n",
    "p_0 = 0.5  # hypothesized proportion (under H0)\n",
    "\n",
    "# Calculate the z-statistic\n",
    "z_stat = (p_hat - p_0) / np.sqrt(p_0 * (1 - p_0) / n_students)\n",
    "\n",
    "# Calculate the p-value (for a one-tailed test, since H1: p > 0.5)\n",
    "p_value = 1 - norm.cdf(z_stat)\n",
    "\n",
    "print(f\"Sample proportion (p̂): {p_hat}\")\n",
    "print(f\"Z-statistic: {z_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05  # significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis (H₀).\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H₀).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
